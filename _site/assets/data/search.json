[
  
  {
    "title"    : "FitNet",
    "category" : "Computer Vision, Knowledge Distillation",
    "url"      : "/computer%20vision/knowledge%20distillation/2022/05/09/fitnet/",
    "date"     : "May 9, 2022",
    "excerpt"  : "fitnet 논문 링크 \n많이 쓰이는 딥러닝 모델은 inference time에 많은 시간이 소요된다. 그리고 파라미터 수가 많아서 많은 메모리도 필요하다.\n이러한 이유로 Knowledge Distillation을 사용한다.\n하지만 이전의 연구들은 더 얕은 네트워크에 적용하지 않아 속도면에서 아쉬운 점이 있었다.\n따라서 이 논문에서 더 얕은 네트워크를 사용하여 compression하는 방법을 제공한다.\n\nMethod\nReview of Kn...",
    "content"  : "fitnet 논문 링크 \n많이 쓰이는 딥러닝 모델은 inference time에 많은 시간이 소요된다. 그리고 파라미터 수가 많아서 많은 메모리도 필요하다.\n이러한 이유로 Knowledge Distillation을 사용한다.\n하지만 이전의 연구들은 더 얕은 네트워크에 적용하지 않아 속도면에서 아쉬운 점이 있었다.\n따라서 이 논문에서 더 얕은 네트워크를 사용하여 compression하는 방법을 제공한다.\n\nMethod\nReview of Knowledge Distillation\n\n이전 연구(Hinton &amp;amp; din, 2014)에선 student network가 학습할 때 제공된 label뿐만 아니라 teacher network의 output까지 학습하게 한다.\n는 teacher의 output, 는 student의 output이라 하자.\n또한 는 true label과 유사하기 떄문에 τ를 사용하여 soften시킨다.\n\n\n$$P^{\\tau}_T=softmax(a_T/\\tau), P^{\\tau}_S=softmax(a_S/\\tau)$$  \n\n\nstudent network는 다음을 최적화하는 것이 목표이다.\n\n$$L_{KD}(W_S)=H(y_{true}, P_S) + \\lambda H(P^{\\tau}_T, P^{\\tau}_S)$$  \n\n\nH는 cross entropy이고, λ는 두 cross entropy의 균형을 맞추는 hyper parameter이다.\n\nHint based Training\n저자는 DNN을 학습시키기 위해 hint와 guide layer라는 것을 도입했다. hint는 student의 학습을 도와주기 위한 teacher의 hidden layer이다. \n또한 guide layer는 teacher의 hint layer로부터 배우는 student의 hidden layer이다.\n저자는 guide layer가 teacher의 hint layer를 학습하도록 목표를 잡았다.\n이때 hint layer와 guide layer는 teacher와 studnet의 middle layer로 설정했다.\n그리고 guide layer는 hint와 차원이 맞지 않기 떄문에 regression layer를 추가했다.\n\n\n$$L_{HT}(W_{Guided}, W_r) = 1/2||u_h(x;W_{Hint}) - r(v_g(x;W_{Guided}); W_r)||^2$$  \n\n\n는 각각teacher와 student의 nested funsiton이고 는 teacher와 student의 parameter이다.\n\nregression layer를 fully connected layer로 설정할 수 있지만 파라미터수가 많아지므로 cnn layer를 사용하여\n 에서 로 줄일 수 있었다.\n\nTraining Method\nFitNet(논문에서 제안한 방법으로 학습된 네트워크)은 teacher가 student를 가르치는 방법으로 다음과 같이 직관적인 학습과정을 거친다.\n\n\n  학습된 teacher network와 random initialized된 student network를 준비한다.\n  hint와 guide layer를 가지고 regressor를 학습시킨다.\n  hint와 regressor를 사용해 guide를 학습시킨다. 이 때 studnet의 학습이 일어난다.\n\n\n이에대한 알고리즘은 다음과 같다.\n\n\nResult\n결과는 다음과 같다.\n\n\n느낀점\nfeature map base로 학습시킨다는 의도는 좋았다. 많은 논문이 이를 따른다는 것에 의미가 있다. 하지만 regressor를 따로 학습시킨다는 것에 의문이 든다.\nregressor를 학습시킬떄는 teacher는 의미있는 representation이 있지만 student는 의미있는 representation을 가지고 있지 않다. \n따라서 각각의 representation space의 변환이 적절하게 되었는지는 의문이다.\n"
} ,
  
  {
    "title"    : "[OSAM] 3. 이제 끝나는 건가?",
    "category" : "OSAM",
    "url"      : "/osam/2021/11/11/osam-3/",
    "date"     : "November 11, 2021",
    "excerpt"  : "해커톤이 끝나간다. 그런데 프론트는 아직 안 끝났고 결과물 제출 일주일 전에 갑자기 인스타그램 연동, 지속적인 학습 및 모델 업데이트, 로그를 넣자고 한다.\n나는 반대했지만 우리와 비슷한 아이디어를 가진 다른팀이 있어 다수의 의견대로 진행하기로 했다.\n\n\n\n그렇게 로그를 추가하고…\n\n\n\n인스타봇을 추가하고…\n\n\n\n대충 이런 REST API서버를 장고로 만들고…\n\n이제 진짜 끝났다.\n\n이제 진짜 끝\n\n\n \n  \n  \n  \n \n\n \n  \n  ...",
    "content"  : "해커톤이 끝나간다. 그런데 프론트는 아직 안 끝났고 결과물 제출 일주일 전에 갑자기 인스타그램 연동, 지속적인 학습 및 모델 업데이트, 로그를 넣자고 한다.\n나는 반대했지만 우리와 비슷한 아이디어를 가진 다른팀이 있어 다수의 의견대로 진행하기로 했다.\n\n\n\n그렇게 로그를 추가하고…\n\n\n\n인스타봇을 추가하고…\n\n\n\n대충 이런 REST API서버를 장고로 만들고…\n\n이제 진짜 끝났다.\n\n이제 진짜 끝\n\n\n \n  \n  \n  \n \n\n \n  \n  \n  \n \n\n \n  \n  \n  \n \n\n  \n**어플 완성!!!!**  \n\n제출 2일전에 드디어 완성시키고 그 다음 문서를 작성했다. 주최측에서 [레포지토리](https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary) [README](https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary)로 개발문서를 만들라고 해서 README에 문서를 떄려 박았다.\n\n\n[github readme](https://github.com/osamhack2021/AI_APP_WEB_Canary_Canary), \n[ppt](https://docs.google.com/presentation/d/1s4Sa52awVV3G2vbk3Qd4I2jI2PlZm4a3-0RNrLDudvI/edit?usp=sharing)\n\n## 결과\n11월 8일 최종결과 발표였는데 국방부 승인이 나지 않아서 11월 11일에 나왔다.\n결과는 두구둑두구....  \n  \n  \n\n해군참모총장상을 받았다. 8등 정도 한 것 같았는데 아쉬웠지만 다들 프로젝트를 처음한다고 하니까 다행이라 생각한다.\n5주 동안 고생한 나에게 박수를!!!!\n\n"
} ,
  
  {
    "title"    : "[OSAM] 2. computer vision 개발 과정",
    "category" : "OSAM, Computer Vision",
    "url"      : "/osam/computer%20vision/2021/10/29/osam-2/",
    "date"     : "October 29, 2021",
    "excerpt"  : "이 포스트는 이전 포스트를 읽으면 이해하기 더 쉽다.\n[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계\n\nObject Detection 모델을 선택하는데 몇 가지 기준을 세웠다.\n\n1 . model train과 evaluation이 빨라야 한다.\n\n  memory를 적게 잡아먹어야 한다.\n  성능이 나쁘지 않아야 한다.\n\n\n그렇게 SSD, SSDLite, EfficientNet, YOLOv5가 후보에 올랐다.\nEfficientNet은...",
    "content"  : "이 포스트는 이전 포스트를 읽으면 이해하기 더 쉽다.\n[OSAM] 1. 팀 결정 및 주제&amp;amp;시스템 설계\n\nObject Detection 모델을 선택하는데 몇 가지 기준을 세웠다.\n\n1 . model train과 evaluation이 빨라야 한다.\n\n  memory를 적게 잡아먹어야 한다.\n  성능이 나쁘지 않아야 한다.\n\n\n그렇게 SSD, SSDLite, EfficientNet, YOLOv5가 후보에 올랐다.\nEfficientNet은 학습시간이 너무 오래걸렸고, SSD는 성능이 매우 낮아 YOLOv5를 선택했다.\n\nDataset준비\n우리가 만들 모델은 군사 시설, 장비, 용품 등을 인식하는 문제였다.\n데이테셋을 찾아보려고 해도 우리가 원하는 데이터를 찾기 어려웠다.\n하지만 다행이 kaggle에 ImageNet Object Localization Challenge가 있었고, \n이 데이터 + 직접 크롤링한 데이터를 이용하기로 했다.\n\n모델 학습\n\n일단 다들 알다싶이 precision과 recall은 trade off관계에 있다.\n그리고 앞에서 말했 듯 우리가 만들고자 하는 것은 군사보안에 관한 것이다.\n물체가 잘못 인식되어서 모자이크가 잘못 쳐지는 것 보다 군관련 사항을 모자이크를 못 하는 것이 치명적인 오류이다.\n따라서 평가지표를 recall로 잡고 개발을 진행하기로 했다.\n먼저 vanilla yolov5를 이용하여 학습을 진행했고 다음과 같은 결과가 나왔다.\n\n\n  \n    \n      enhance\n      model\n      precision\n      recall\n      mAP_0.5\n      mAP_0.5:0.95\n    \n  \n  \n    \n      Vanilla\n      yolov5m6\n      0.602\n      0.651\n      0.671\n      0.535\n    \n  \n\n\n문제점 분석\n데이터\n\n1차적으로 만든 데이터셋의 특성이고 다음과 같이 분석했다.\n\n\n  bounding box는 한 class당 500개 정도이다.\n  bounding box 중심의 위치는 대개 정중앙이다.\n  bounding box가 이미지의 크거나 대부분을 차지한다.\n\n\n모델\n경량모델의 문제점이 그대로 나타났다. 모델이 가벼워 training set에 overfitting이 잘 되었고, 모델 자체의 성능도 낮았다.\nyolov5x6와 같은 같은 계열의 무거운 모델을 쓸 수 있지만 그러면 서비스 자체가 느려질 것이었다\n\n해결방법\n\n해결방안 1 - 데이터 추가\n\n\n  \n   Orignal Dataset\n   Add more data\n  \n  \n   \n   \n  \n \n\n누가 뭐라고 하던 데이터가 많으면 최고다.\nclass당 500개의 box는 말도 안되는 개수라 직접 imgenet dataset에서 annotation을 해서 수를 1200개 이상으로 늘렸다.\n그러자 bounding box의 중심도 많이 퍼졌고, small object도 많이 생겨났다.\n\n해결방안 2- augmentation 방법 변경\n\n\n  \n   기존\n   변경\n  \n  \n   \n   \n  \n \n\n기존 데이터셋의 문제점이 물체의 중심이 이미지 정가운데이고, 물체가 이미지의 대부분을 차지한다 였다.\n이 문제점을 해결하기 위해 yolov5에 있는 mosaic augmentation이 적절했으나 이는 부족했다.\n따라서 우리는 lagacy code에 있는 masaic_9 augmentatio을 사용하기로 했다.\n\n해결방안 3 - knowledge distillation(paper link)\n\n\n\n경량화 기법 중 하나인 knowledge distillation을 사용했다. \nyolov5x6를 teacher model로 yolov5m6를 student model로 knowledge distillation을 진행하면 overfitting을 막아주고 성능이 높아 질 것이다\n\n결과\n\n\n  \n    \n      enhance\n      model\n      precision\n      recall\n      mAP_0.5\n      mAP_0.5:0.95\n    \n  \n  \n    \n      Before add dataset\n      yolov5m6\n      0.602\n      0.651\n      0.671\n      0.535\n    \n    \n      None (Add dataset)\n      yolov5m6\n      0.736\n      0.779\n      0.815\n      0.599\n    \n    \n      mosaic_9 50%\n      yolov5m6\n      0.756\n      0.775\n      0.809\n      0.602\n    \n    \n      mosaic_9 100%\n      yolov5m6\n      0.739\n      0.813\n      0.806\n      0.594\n    \n    \n      knowledge distillation\n      yolov5m6\n      0.722\n      0.822\n      0.807\n      0.592\n    \n  \n\n\n\n \n  Original Image\n  Result Image\n \n \n  \n  \n \n\n\n결과적으로 recall이 17.1%p를 올리는 결과를 냈다. recall을 높이기 위해 precision이 조금 낮아졌다는 것이 아쉬였다.\n사용했던 knowledge distillation code는 여기에 있다.\nyolov5-knowledge-distillation\n"
} ,
  
  {
    "title"    : "[OSAM] 1. 팀 결정 및 주제&amp;시스템 설계",
    "category" : "OSAM",
    "url"      : "/osam/2021/10/29/osam-1/",
    "date"     : "October 29, 2021",
    "excerpt"  : "군대에서 재미없는 나날을 보내고 있었는데 동아리형으로부터 OSAM에 꼭 나가보라는 이야기를 들었다.\n근데 OSAM이 뭐지??\n\n\n\n오 신기한거다라고 생각하며 참여했다.\n\n1차 아이디어\n나는 지금까지 computer V\\vision을 공부하고 있었기 때문에 관련 주제를 선정했다.\n다들 알다싶이 요즘 군대에선 핸드폰을 쓸 수 있다. 하지만 카메라는 예외이다. 나는 항상 그것이 의문스러웠다. 병사는 보안을 위반할 보안도 없는데…\n아무튼 병사들도...",
    "content"  : "군대에서 재미없는 나날을 보내고 있었는데 동아리형으로부터 OSAM에 꼭 나가보라는 이야기를 들었다.\n근데 OSAM이 뭐지??\n\n\n\n오 신기한거다라고 생각하며 참여했다.\n\n1차 아이디어\n나는 지금까지 computer V\\vision을 공부하고 있었기 때문에 관련 주제를 선정했다.\n다들 알다싶이 요즘 군대에선 핸드폰을 쓸 수 있다. 하지만 카메라는 예외이다. 나는 항상 그것이 의문스러웠다. 병사는 보안을 위반할 보안도 없는데…\n아무튼 병사들도 조금은 자유롭게 카메라를 쓸 수  사람을 제외한 모든 부분은 날려버리는 프로그램을 만드려고 했다.\n\n\n\n다른 사람이 만든거지만 이런 느낌이랄까? 그렇게 개발계획서를 작성하고 팀을 모집하다가 메일이 하나 왔다.\n\n\n처음 메일을 받고 팀을 합칠까 고민을 했고, 비슷한 아이디어라 나중에 평가 받을 때 수상을 못할 확률이 있어서 팀에 합류하기로 했다.\n\n2차 아이디어\n그렇게 나는 ‘카나리아’팀에 들어갔다. 대충 어플의 컨셉을 말하자면 다음과 같다.\n\n🐤카나리아 : 모두를 위한 군사보안 경보 시스템\n\n\n\n \n \n\n\nCanary는 머신러닝을 활용하여 사진 안의 보안 위반 가능성이 있는 요소를 식별하고, 자동 모자이크 처리를 하고, 이를 사용자에게 경고해주는 통합 보안 경보 시스템입니다. \nCanary App, Canary in Instagram, Admin logweb으로 구성되어 있으며, 앱에서 처리된 사진에는 QR코드가 들어가 처리 여부를 쉽게 식별할 수 있습니다.\n\n🗂️프로젝트 소개\n본 프로젝트는 사진의 보안 내용을 제거하는 기능과 그러한 기능을 가진 카메라를 제공함으로서,\n\n  군 내에서 카메라를 사용 가능하게 함과 동시에,\n  SNS에 올릴 사진의 보안 위반 가능성을 경고하여 사용자가 자발적으로 보안을 준수 할 수 있게 합니다.\n  또 현재 SNS올라가 있는 게시물을 검사를 해 보안에 대한 경각심을 일으킬 수 있습니다.\n\n\n기간은 한 달… 한 달안에 끝낼 수 있을까 모르겠지만 최선을 다하면 되겠지 하면서 시작했다.\n\n설계\n예상 사용자 설정과 시스템 설계는 전에 만들어 놓은 문서로 때우겠다. 이거면 다 알아보겠지 하면서 말이다\n\n페르소나\n\n\n시나리오\n\n#1\n막 자대배치를 받은 안준호 이병. 택배로 스마트폰을 받는다.\n\n\n  안준호 이병은 처음으로 어플리케이션을 실행한다. \n0-1. 군번, 이름, 계급을 입력하여 자신의 정보를 저장한다.\n  드디어 스마트폰을 받아 두근대는 마음으로 사진을 찍기 위해 어플리케이션을 켠다.\n  촬영 모드로 들어가서 카메라를 켠 후 생활관 TV를 배경으로 사진을 찍는다.\n  잠시 후, TV 모니터가 모자이크 된 사진과 함께 경고 문구가 출력된다.\n  사진 저장 시 사진에 QR코드가 새겨진다. QR코드에는 안준호 이병의 군번이 암호화되어 들어간다.\n  모자이크가 된 사진을 SNS에 올려 자랑한다.\n\n\n#2\n긴 군생활을 끝내고 드디어 전역한 최종훈 병장. 같이 전역하는 동기들과 기념 사진을 찍는다.\n\n\n  최종훈 병장과 동기들은 부대 앞에서 기념 사진을 촬영한다.\n  SNS에 이 글을 게시하기 전, 최종훈 병장은 혹시 사진에 군사보안 위반은 없는지 걱정된다.\n  어플리케이션을 실행한 후, 방금 전 찍은 사진을 갤러리에서 선택한다.\n  잠시 후, 부대마크와 군 표지판 부분이 모자이크 된 사진과 함께 경고 문구가 출력된다.\n  사진 저장 시 사진에 QR코드가 새겨진다. QR코드에는 최종훈 병장의 군번이 암호화되어 들어간다.\n  최종훈 병장은 안심하면서 SNS에 사진을 업로드 한다.\n\n\n#3\n예비군 유시진 씨. 인스타그램에 올렸던 군대 사진들을 본다.\n\n\n  유시진 씨는 인스타그램에 올렸던 훈련 사진을 본다.\n  옛날 사진을 보던 중, 한 사진에 탱크가 찍힌 것을 본다.\n  Canary Instagram bot에 이 사진을 검토해 줄 것을 메시지로 요청한다.\n  잠시 후, 탱크가 모자이크 된 사진과 함께 경고 문구를 메시지로 받는다.\n  유시진 씨는 SNS 사진을 수정한다.\n\n\n시스템 흐름도\n\nUser-case Diagram\n\n\nSequence Diagram\n\n\nArchitecture\n\n\n시작\n두둥… 이제 개발을 시작한다. 한 달 후 어떻 결과물이 나올까 기대된다.\n\n"
} ,
  
  {
    "title"    : "[AutoML] NASNet",
    "category" : "Computer Vision, Neural Architecture Search",
    "url"      : "/computer%20vision/neural%20architecture%20search/2021/01/27/NASNet/",
    "date"     : "January 27, 2021",
    "excerpt"  : "2017년에 NASNet이 나온 이후 2018년 부터 AutoML의 시대가 열렸습니다.\nMnasNet, MobileNet V3, EffientNet등 여러 경량화 네트워크들은 NAS(Neural Architecture Search)을 사용했고, 모바일쪽 네트워크들은 많이 NAS를 사용하고 있습니다.\n이번에 소개할 논문은 Neural Architecture Search with Reinforcement Learning를 소개해드리겠습니다.\n...",
    "content"  : "2017년에 NASNet이 나온 이후 2018년 부터 AutoML의 시대가 열렸습니다.\nMnasNet, MobileNet V3, EffientNet등 여러 경량화 네트워크들은 NAS(Neural Architecture Search)을 사용했고, 모바일쪽 네트워크들은 많이 NAS를 사용하고 있습니다.\n이번에 소개할 논문은 Neural Architecture Search with Reinforcement Learning를 소개해드리겠습니다.\n\nNeural Architecture Search은 RNN을 사용하여 model description을 생성하고 생성된 네트워크를 학습합니다.\n그리고 validation set을 이용하여 accuracy를 구하고 이를 reward로 만듭니다.\n후에 이 reward를 갖고 policy gradient를 구해 controller를 업데이트 시킵니다.\n\n\n\nGenerate Model Description\nNAS에서 controller가 model description을 만듭니다.\nmodel description을 갖고 model을 만든 다음 학습을 시킵고 수렴이 되면 validation set으로 accuracy를 측정하게 됩니다.\ncontroller RNN의 파라미터 는 validation accuracy의 평균을 이용해 최적화 시킵니다.\n\n\n\nTraining With REINFORCE\ncontroller를 학습시키려면 우리는 loss 함수를 정의해야합니다.\ncontroller에서 만들어진 네트워크를 child network라고 하겠습니다. \n그리고 child network를 구성하기 위한 action들을 , 학습된 child network의 validation accuracy를 R이라고 할 때 는 다음과 같습니다.\n\n\n$$J(\\theta_c)=E_{P(a_{1:T};\\theta_c)}[R]$$\n\n\nreward signal인 R은 미분 불가능하기 떄문에 REINFORCE라는 policy로 update를 합니다.\n이는 다음과 같은 식으로 만들 수 있고\n\n\n$$\\nabla_{\\theta_c} J(\\theta_c) = \\sum_{t=1}^{T}E_{P(a_1:T; \\theta_c)}[\\nabla_{\\theta_c} logP(a_t | a_{(t-1):1}; theta_c) R]$$  \n\n\n따라서 위의 식은 다음과 같이 근사될 수 있습니다.\n\n\n$$\\frac{1}{m} \\sum_{k=1}^{m} \\sum_{t=1}^{T} \\nabla_{\\theta_c} logP(a_t | a_{(t-1):1}; {\\theta_c}) R_k$$  \n\n\nm은 한 배치에 controller가 만들 child network의 개수이고 k번째 child network의 validation accuracy를 이다.\n위의 식은 불편 추정치이지만 분산이 크기 때문에 분산을 줄이기 위해서 다음과 같은 식을 씁니다.\n\n\n$$\\frac{1}{m} \\sum_{k=1}^{m} \\sum_{t=1}^{T} \\nabla_{\\theta_c} logP(a_t | a_{(t-1):1}; {\\theta_c}) (R_k-b)$$  \n\n\nb는 이전 네트워크들의 validation accuracy의 지수이동평균입니다.\n\nSkip Connection\nGoogleNet, ResNet같은 네트워크들은 skip connection을 통해 성능을 높힙니다.\n이와 같이 NASNet에서도 skip connection을 생성하기 위해 다음과 같이 설정했습니다.\n\nlayer N에서 이전의 layer으로 부터 skip connection이 있는지를 결정하는 N-1개의 content-based sigmoid(anchor point)를 추가합니다.\n\n\nP(Layer j is an input to layer i) = $$sigmoid(v^T tanh(W_{prev} * h_j + W_{curr} * h_i))$$  \n\n\n는 j번째 layer의 controller의 hiddenstate이고 그 값은 0 부터 N-1까지 가질 수 있다.\n\n\n\n하지만 이렇게 연결하다보면 구조가 망가지는 경우가 있기 때문에 다음과 같은 규칙을 정한다.\n\n\n  input layer로 쓴 layer는 다른 layer의 input layer가 되지 않는다.\n  skip connection이 안되어있는 layer를 다 final layer에 connection을 만든다.\n  만약 skip connection시 layer의 size가 다르면 zero padding을 한다.\n\n\nResult\n\n\nOpinion\n손수 만든 네트워크들에 비해 그렇게까지 정확도가 높다고 말할 수는 없지만 그래도 AutoML로 만들었다는 것에 의의가 있는 것 같습니다.\n그리고 실험결과를 볼 때 filter들은 직사각형이 많다라는 말이 있는데 무작위로 뽑아도 직사각형이 될 확률이 높아서 그렇지 않을까 생각합니다.\n"
} ,
  
  {
    "title"    : "[Python] 우선순위 큐 (heapq vs priority queue)",
    "category" : "Data Structure, Python",
    "url"      : "/data%20structure/python/2021/01/10/heapq-vs-priority-q/",
    "date"     : "January 10, 2021",
    "excerpt"  : "파이썬에서는 유용한 자료구조 라이브러리를 제공합니다. \n그 중에 하나는 우선순위 큐(priority queue)입니다.\n\n우선순위 큐란?\n우리는 많은 경우에서 우선순위를 만납니다.\n응급실의 예를 들어보겠습니다.\n다음의 환자들이 있습니다. \n스케쥴링을 이야기할 것이 아니기 때문에 치료 시간은 0으로 가정하겠습니다.\n\n\n  5분 이내 치료해야할 사람\n  10분 이내 치료해야할 사람\n  2분 이내 치료해야할 사람\n\n\n우리는 위의 환자의 치료 순...",
    "content"  : "파이썬에서는 유용한 자료구조 라이브러리를 제공합니다. \n그 중에 하나는 우선순위 큐(priority queue)입니다.\n\n우선순위 큐란?\n우리는 많은 경우에서 우선순위를 만납니다.\n응급실의 예를 들어보겠습니다.\n다음의 환자들이 있습니다. \n스케쥴링을 이야기할 것이 아니기 때문에 치료 시간은 0으로 가정하겠습니다.\n\n\n  5분 이내 치료해야할 사람\n  10분 이내 치료해야할 사람\n  2분 이내 치료해야할 사람\n\n\n우리는 위의 환자의 치료 순위를 어떻게 정해야합니까?\n3 -&amp;gt; 1 -&amp;gt; 2순으로 치료해야 할 것입니다. \n위의 상황을 해결해주는 자료구조가 우선순위 큐입니다.\n\n구현 방법은 이 글에서 다루지 않겠습니다.\n\n파이썬 라이브러리\n파이썬에서는 heapq, PriorityQueue로 우선순위 큐를 지원합니다.\n사용법은 다음과 같습니다.\n\nheapq\nimport heapq\n\npq = []\n\nheapq.heappush(pq, 1)\nheapq.heappush(pq, 3)\nheapq.heappush(pq, 2)\n\nheapq.heappop(pq) # 1\nheapq.heappop(pq) # 2\nheapq.heappop(pq) # 3\n\n\nPriorityQueue\nfrom queue import PriorityQueue\n\npq = PriorityQueue()\n\npq.put(1)\npq.put(3)\npq.put(2)\n\npq.get() # 1\npq.get() # 2\npq.get() # 3\n\n\n다음과 같은 의문이 들 수 있습니다.\n똑같은 역할을 하는데 과연 두 라이브러리들은 무엇이 다를까?\n\nPriorityQueue는 객체이고 heapq는 여러 함수들이 들어있는 파일입니다.\n맞는 말이긴 합니다.\n\n정확하게 말하면 PriorityQueue은 lock을 제공하여 thread-safty class입니다.\n반면에 heapq는 list를 사용하기 때문에 thread-safty class가 아닙니다.\n\n그래서 그런지 몰라도 실행시킬 때 실행속도가 차이가 납니다. (제 추측…)\n\n실행속도 비교\nt2.micro에서 실험을 진행했습니다.\n\nDuration of PriorityQueue 0.980911\nDuration of heapq         0.175374\n\n\n코드\nfrom queue import PriorityQueue\nimport heapq\nimport random\nfrom time import time\n\nnums = [random.random() for _ in range(100000)]\n\npriority_queue = PriorityQueue()\npq = []\n\nstart = time()\nfor i in range(len(nums)):\n    priority_queue.put(nums[i])\n\nfor i in range(len(nums)):\n    priority_queue.get()\nend = time()\nprint(f&#39;Duration of PriorityQueue {end - start:.6f}&#39;)\n\n\nstart = time()\nfor i in range(len(nums)):\n    heapq.heappush(pq, nums[i])\n\nfor i in range(len(nums)):\n    heapq.heappop(pq)\nend = time()\nprint(f&#39;Duration of heapq         {end - start:.6f}&#39;)\n\n\n마치며\n사실 이 라이브러리는 백준 보석도둑을 풀면서 알게되었습니다.\n알고리즘문제를 풀때 heapq를 써야지 PriorityQueue를 쓰면 시간초과가 결렸습니다.\n다들 저와 같은 실수를 하지 않길 빌면서 이만 가보도록 하겠습니다.\n알고리즘 어렵다!\n"
} ,
  
  {
    "title"    : "학부생이 본 SENet",
    "category" : "Computer Vision",
    "url"      : "/computer%20vision/2021/01/10/SENet/",
    "date"     : "January 10, 2021",
    "excerpt"  : "ImageNet Challenge에서 많은 네트워크들이 제안되었습니다. \nResNet, VGG, Inception그 예입니다. \n하지만 이러한 네트워크들은 크기가 크다는 것이 단점이었습니다.\nILSVRC 2017에서 우승한 SENet은 squeeze-and-excitation이라는 구조로 기존 모델에서 추가적으로 성능을 높일 수 있는 Block구조를 만들었습니다.\n\n이 연구에서는 네트워크의 구조가 아닌 channel들의 관계들에 집중했습니...",
    "content"  : "ImageNet Challenge에서 많은 네트워크들이 제안되었습니다. \nResNet, VGG, Inception그 예입니다. \n하지만 이러한 네트워크들은 크기가 크다는 것이 단점이었습니다.\nILSVRC 2017에서 우승한 SENet은 squeeze-and-excitation이라는 구조로 기존 모델에서 추가적으로 성능을 높일 수 있는 Block구조를 만들었습니다.\n\n이 연구에서는 네트워크의 구조가 아닌 channel들의 관계들에 집중했습니다.\n이전의 연구들에선 channel간의 correlation을 spatial한 정보들을 무시하고 1x1 convolutoin등과 같이 계산했습니다.\n하지만 SENet에서는 global information을 이용해 명시적으로 channel간에 non-linear dependency를 계산하여 모델의 성능을 높였습니다.\nSqueeze-and-excitation구조는 기존의 네트워크에 쉽게 적용하여 성능을 높일 수 있어서 더욱 유용한 구조입니다.\n\nSqueeze-and-excitation Block\nSENet에서 comvolution들의 channeel들의 관계는 receptive field에 제한적이고 암묵적임으로 이를 명시적으로 모델링해주면 정보가 많은 특징들에 더 민감해지면서 성능이 높아진다고 말하고 있습니다.\n\n\n\nSqueeze: Global Information Embedding\n앞써 말했듯 convolutoin은 변환이 receptive field에 국한되어있어서 receptive field밖의 contextual information은 고려할 수 없습니다.\n따라서 global spatial information을 channel discriptor로 만들기 위해 Squeeze연산을 수행합니다. \n이 과정은 간단히 global average pooling을 이용했습니다.\n\nChannel discriptor인 은 U를 spatial dimension으로 만큼 압축해서 얻을 수 있습니다.\n따라서 c번째 체널의 z성분은 다음과 같이 계산될 수 있습니다.\n\n\n\ntransformation U는 전체적인 이미지의 통계값인 local descriptor로 생각될 수 있고 global average pooling으로 구현할 수 있습니다.\n\nExcitation: Adaptive Recallibration\nChannel discriptor를 만들었으면 이제 체널의 관계들을 계산해줘야 합니다. \n여기서 고려해야할 사항이 두 개가 있습니다.\n\n  체널들의 관계를 포착하기 위해 flexible해야합니다.\n  체널들을 강조하기 위해 non-mutually-exclusive한 정보를 학습해야 합니다. (one-hot같은 mutually-exclusive면 안됩니다.)\n\n\n따라서 여기서는 sigmoid를 사용했습니다.\n\n\n는 ReLU함수를 의미하고,  그리고 \n모델의 복잡도와 일반화를 위해서 bottleneck이 있는 두 개의 FC Layer를 이용했고 reduction ratio r을 갖습니다.\n\n따라서 최종 출력은\n\n 이고 은 channel-wise multiplication을 의미합니다.\n\nexcitation operator는 channel discriptor인 z를 channel weight으로 연결해주는 연산입니다.\n\n이 두 가지 연산을 통해 적은 추가적인 연산량과 모델크리고 model capacity를 늘릴수 있습니다.\n\nSE-Inception ans SE-ResNet\n\n다음과 같이 Residual Block에 Squeeze-and-excitation구조를 적용함으로써 기존의 네트워크의 성능을 높일 수 있습니다.\n\nResult\nSE-Inception ans SE-ResNet의 다음과 같습니다.\n\n그리고 실험 결과는 다음과 같습니다.\n\n\n마치며\nSqueeze-and-excitation Block은 MobileNet V3등 여러 네트워크에서 사용하는 강력한 구조입니다. \nEffientNet과 같이 새로운 네트워크 구조를 제안할까가 아니라 기존 네트워크의 부족한 점이 무엇인지 생각한 다음 실험을 하여 더 좋은 성능을 얻었다는 것이 신기했습니다.\n특히 이전 연구들과 달리 channel간의 dependency를 global한 contextual information까지 확장했다는 것에 대해 의의가 있는 것 같습니다.\n"
} ,
  
  {
    "title"    : "[네트워크 경량화] EffientNet",
    "category" : "Computer Vision, Light Weight",
    "url"      : "/computer%20vision/light%20weight/2021/01/03/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B2%BD%EB%9F%89%ED%99%94-EffientNet/",
    "date"     : "January 3, 2021",
    "excerpt"  : "이번에 소개해드릴 논문은 EffientNet입니다. \nEffientNet은 ICML 2019에 나왔고, 저자는 이전에 MnasNet을 발표한 적이 있습니다.\n\n\n실험 결과\n\nIntroduction\nImageNet Dataset이 나온 이후에 여러 classification모델이 제안되었습니다.\nVGG이후 ResNet부터 네트워크를 깊게 쌓음으로써 정확도를 올리게 되었고, GPipe경우에는 base model보다 4배를 크게 만듬으로써 Im...",
    "content"  : "이번에 소개해드릴 논문은 EffientNet입니다. \nEffientNet은 ICML 2019에 나왔고, 저자는 이전에 MnasNet을 발표한 적이 있습니다.\n\n\n실험 결과\n\nIntroduction\nImageNet Dataset이 나온 이후에 여러 classification모델이 제안되었습니다.\nVGG이후 ResNet부터 네트워크를 깊게 쌓음으로써 정확도를 올리게 되었고, GPipe경우에는 base model보다 4배를 크게 만듬으로써 ImageNet에서 우승했습니다.\n\n이와 같이 보통은 depth를 늘리면서 정확도를 올리고 width를 늘리거나 resolution을 늘리면서 정확도를 올리게 됩니다.\n\n이러한 연구들을 보다 보면 depth, width, resolution이렇게 3가지를 균형을 맞추면 더 성능이 좋아지지 않을까?” 이런 생각을 갖게될 것입니다.\n직관적으로 생각하면 큰 resolution의 이미지를 넣으면 receptive field가 커지게되고 그 패턴을 캡쳐하기 위해 channel수도 늘어야될 것입니다.\nEffientNet은 이러한 아이디어를 갖고 compound scaling method를 제안하며 기존의 네트워크의 성능을 올리게 되었고, 새로운 state-of-art모델을 제안했습니다.\n\nCompund Model Scaling\nComvolution Filter는 다음과 같이 정의할 수 있습니다.\n\n$$Y_i=\\mathcal{F}_i(X_i)$$  \n\n는 output tensor, 는 input tensor, 는 operator입니다.\n\n이것을 개의 Layer를 쌓는다고 하면 다음과 같이 표시할 수 있습니다.\n\n$$\\mathcal{N} = \\bigodot_{i=1...s} \\mathcal{F}^{L_i}(\\mathrm{X}_{&amp;lt;H_i, W_i, C_i})$$ \n\n는 가 번 반복되는 notation입니다.\n\n이렇게 간단한 식으로 표시했지만 를 각각 조절해야하기 때문에 Design Space가 너무 넓어서, design space를 좁히기 위해 다음과 같이 constant ratio를 사용하여 표현했고 다음의 수식을 optimization을 하는 것이 목표로 잡을 것 입니다.\n\n\n$$max_{d, w, r} \\mathcal{Accuracy}(\\mathcal{N}(d, w, r))$$  \n$$s.t \\mathcal{N}(d, w, r) = \\bigodot_{i=1..s} \\hat{\\mathcal{F}}_i^{d ⋅ \\hat{L}_i}(X_{r ⋅ \\hat{H}_i, r ⋅ \\hat{W}_i, w /cdot \\hat{C}_i})$$  \n\n$$Memory(\\mathcal{N}) \\leq target_memory$$  \n$$FLOPS(\\mathcal{N}) \\leq target_flops$$  \n\n\n여기서 는 각각 Base model의 parameter를 나타낸 것입니다.\n\nScaling Dimensions\n자 이제 문제는 depth, width, resolution의 조합이 아닌, linear equation의 variacnce인 d, w, r를 조절하는 문제입니다. 먼저 각각의 요소의 영향을 살펴보면 다음과 같이 볼 수 있습니다.\n\n\nDepth(d)\n직관적으로 더 깊은 ConvNet은 더 풍부하고 복잡한 feature를 찾을 수 있지만, 더 깊은 ConvNet은 vanishing gradient problem 등과 같은 문제 때문에 학습하기 힘듭니다.\n\nWidth(w)\n더 큰 width를 갖는 네트워크는 또한 더 풍부하고 복잡한 feature를 찾을 수 있지만, width가 크지만 네트워크가 깊지가 않으면 high level feature를 잡을 수 없습니다.\n\nResolution(r)\n화질이 좋은 이미지는 더욱 세세한 패턴을 잡을 수 있지만,  이미지 사이즈가 커지면 커질수록 정확도가 증가되는 양은 적어집니다.\n\nObservation 1 - Depth, Width, Resolution를 키우면서 정확도를 높일 수 있지만, 커짐에 따라 정확도 증가율은 낮아집니다.\n\nCompund Scaling\n직관적으로 이미지의 화질이 커지면 네트워크의 깊이가 깊어져야하고 이에 따라 receptive field가 커져야 할 것입니다.\n이러한 생각으로 세 가지 요소를 균형을 잡아가면서 늘려가는 것이 좋을 것입니다.\n\n아래의 그림과 같이 depth와 resolution을 고정시키고 width만 늘리면 정확도 증가율이 낮아집니다.\n\n\nObservation 2 - Observation 1의 한계를 극복하기 위해선 더 좋은 정확도를 위해서는 depth, width, resolution의 균형을 잡으면서 ConvNet의 크기를 키워야합니다.\n\n따라서 EffientNet에서는 새로운 Compund Model Scaling을 제안했습니다.\n\n\n$$depth: d = \\alpha^\\phi$$\n$$width: w = \\beta^\\phi$$\n$$depth: r = \\gamma^\\phi$$\n\n\n보통 ConvNet은 에 비례합니다. \n따라서 이 논문에서는 을 잡고 를 조절해가며 model의 capacity를 높입니다.\n\nEffientNet\n앞에서와 같은 아이디어 제안이 있지만 그래도 좋은 모델을 만드는 것이 좋기 때문에 여기서도 새로운 모델을 제안했습니다.\n을 optimization goal로 놓고 Neural Architecture Search를 해 EffientNet-B0모델을 만들었습니다.\nMain block은 은 squeeze-and-excitation을 추가한 mobile inverted bottleneck입니다.\n\n\nEffientNet-B0의 구조\n\n그리고 다음 두 가지 Step으로 다른 모델을 만든다.\n\n  을 고정시키고 Grid Search로 찾은 값은 EffientNet-B0의 최적의 값은 입니다.\n  다음은 를 키워가면서 EffientNet-B1부터 B7를 만들었습니다.\n\n\nResult\n\n이 표에서 볼 수 있는 것과 같이 같은 Accuracy대비 FLOPS를 많이 줄일 수 있습니다.\n\n\n기존에 있는 네트워크에 compund scaling을 사용하면 정확도가 높아집니다.\n\n마치며\nEffientNet은 기존의 새로운 네트워크의 구조를 제안한 것이 아닌 기존의 네트워크에서 부족한 것이 무엇이 있을까라는 의문을 갖고 연구를 했습니다.\ndepth, width, resolution는 각각 독립적인 것이 아닌 의존적이다는 것을 의식하며 연구해 좋은 성과를 냈다는 것이 좋은 연구방법과 결과라고 생각합니다.\n"
} ,
  
  {
    "title"    : "시각장애인을 위한 약학정보제공 서비스 기획기",
    "category" : "product-design",
    "url"      : "/product-design/2020/01/03/barrier-free/",
    "date"     : "January 3, 2020",
    "excerpt"  : "저는 친구와 함께 시각장애인을 위한 약학정보 서비스를 기획하고 개발했습니다. 서비스를 기획하면서 많은 조사와 인터뷰를 진행해 그 경험을 나누고자 합니다.\n우리는 집에 여러개의 상비약들이 있습니다. 우리는 약상자를 보고 원하는 약을 찾을 수 있지만 시각장애인분들은 찾을 수 있을까요? 저희는 이러한 문제에 주목했고 ‘집에 있는 약을 구분을 할 수 있도록 하자’는 생각으로 서비스를 기획하게 되었습니다.\n\n서비스 기획\n인터뷰의 필요성\n\n\n처음에는...",
    "content"  : "저는 친구와 함께 시각장애인을 위한 약학정보 서비스를 기획하고 개발했습니다. 서비스를 기획하면서 많은 조사와 인터뷰를 진행해 그 경험을 나누고자 합니다.\n우리는 집에 여러개의 상비약들이 있습니다. 우리는 약상자를 보고 원하는 약을 찾을 수 있지만 시각장애인분들은 찾을 수 있을까요? 저희는 이러한 문제에 주목했고 ‘집에 있는 약을 구분을 할 수 있도록 하자’는 생각으로 서비스를 기획하게 되었습니다.\n\n서비스 기획\n인터뷰의 필요성\n\n\n처음에는 약학정보 서비스가 아닌 시각장애인을 위한 코디추천 서비스를 기획했습니다. 시각장애인들은 눈이 안보이니 자신이 가지고 있는 옷이 무엇인지 코디를 어떻게 할지 모를거라고 생각했고 여러 논문을 찾아보니 시각장애인들은 기억력의 한계 때문에 비시각장애인보다 옷을 적게 갖고있다고 했습니다. 그래서 시각장애인을 위한 코디추천 서비스를 열심히 기획해 기획안을 들고 한 시각장애인분께 부탁을 드려 인터뷰를 진행하기로 했습니다.\n\n인터뷰를 하던 도중 생각했던 것이 그저 가설이라는 것을 알게되었습니다. 인터뷰 중에 얻었던 내용은 다음과 같습니다.\n\n  자신이 산 옷은 기억한다.\n  보관중인 옷은 만져보면 무슨 옷인지 알 수 있다.\n  옷을 살 떄 세트로 코디를 하면서 사 패션에 대해 그렇게 신경쓰지 않는다.\n\n\n즉 이 서비스에 대한 Needs가 크지 않다는 것입니다.\n\n기획 변경\n소비자의 환경을 파악하라\n\n\n인터뷰를 하던 도중 약학정보를 얻기 힘들다는 이야기를 들었습니다. 그 소리를 듣는 순간 사용할 수 있는 서비스가 없나? 라는 생각이 들었고 앱스토어서 관련서비스를 찾아봤고 iOS 모바일 어플리케이션에서 시각장애인이 사용할 수 있는 서비스가 없었습니다.\n\n\n\n많은 어플리케이션은 label을 만들 때 다음과 같이 만듭니다. 이러한 labeling이 왜 문제가 될까요?\n시각장애인들은 Voice Assistance나 Voice Over와 같은 스크린 리더를 통해 화면을 읽습니다. 이 스크린 리더는 Componentent들의 label이나 text 값을 읽어주기 때문에 이것을 고려를 안하고 앱을 제작하면 시각장애인들이 쓸 수 없게 됩니다.\n\n여러 인터뷰와 회의를 통해 시각장애인을 위한 코디서비스가 아닌 시각장애인을 위한 약학정보제공서비스로 기획을 변경하게 되었습니다.\n\n시각장애인을 위한 UI/UX\nUI에 대해\n다음으로 집약팀은 시각장애인이 선호하는 UI를 파악하려고 노력했습니다. \n\n\n맨 왼쪽부터 ‘행복을 들려주는 도서관’, ‘Be my eyes’, ‘설리번 플러스’ 시각장애인들이 많이 사용하는 3가지 어플리케이션입니다. 이러한 어플리케이션들은 평소에 사용하는 어플리케이션과 UI가 다른 것을 눈치채셨나요? 이 어플리케이션들의 특징은 다음과 같습니다.\n\n\n  글자는 24pt 이상 글자과 배경의 색은 대비되도록\n  단순한 메뉴배치\n\n\n왜 이러한 규칙들이 있을까요? 이 규칙들은 시각장애인들의 어플리케이션 사용방식과 연관이 있습니다.\n\n\n  시각장애인은 크게 약시와 전맹으로 구분됩니다.\n\n\n약시인 시각장애인은 앞이 아예 안보이지는 않기 때문에 스마트폰에 화면 밝기를 최대로 키우고 내장되어있는 돋보기를 사용하여 스마트폰을 눈 가까이에 가져가 스마트폰을 조작합니다. 이를 고려하여 글자는 24pt 이상 글자과 배경의 색은 대비되도록 UI를 구성해 약시인 시각장애인을 고려한 UI를 구성할 수 있습니다.\n\n  전맹인 시각장애인은 스크린리더를 통해 스마트폰 화면을 파악합니다.\n이 스크린 리더는 손으로 스마트폰 화면을 좌우로 넘겨 Component를 변경합니다. 이때 예측가능한 Component 변경을 위해 단순한 메뉴배치를 사용합니다.\n\n\nUX에 대해\n\n\n시각장애인들의 약 구분을 위해 사진을 찍고 찍은 사진을 통해 검색을 하는 기능을 넣기로 했고 이 기능을 만드는 것에 대한 두 가지 방안이 존재했습니다.\n\n\n  바코드로 검색한다.\n  약표지에 적혀있는 글자를 사용해 검색한다.\n\n\n바코드를 사용하면 검색에 대한 정확도를 보장할 수 있습니다. 하지만 집약팀은 시각장애인이 약상자에서 바코드가 어디에 있는지 알 수 없다고 판단해 1번 방법이 아닌 2번 방법을 채택하기로 했습니다.\n\n사용자의 피드백\n제품을 만든 후의 실사용자의 피드백\n자! 이렇게 집약이라는 어플리케이션이 탄생했습니다. 이후 집약팀이 만든 서비스가 실제로 유용한지 확인하기 위해 시각장애인 모임에 연락을 해서 테스트 및 인터뷰를 진행했습니다. 하지만 개발단계에서 예상하지 못한 문제가 발생했습니다.\n\n\n  선천적 시각장애인은 사진찍는 것에 익숙하지 않다.\n\n\n선천적 시각장애인은 사진을 찍어볼 기회가 거의 없습니다. 따라서 피사체와 거리를 얼마나 두어야 하는지, 사진을 어떻게 찍어야하는지 몰라 피사체를 핸드폰에 붙혀서 사진을 찍거나, 카메라 렌즈를 손으로 가려 사진을 찍었습니다.\n\n\n  내가 예상한 어플리케이션 사용방법과 사용자의 어플리케이션 사용 방법이 다르다.\n\n\n검색을 위해 사진을 찍을 때 집약팀은 편한 사용성을 위해 화면중 아무곳이나 클릭하여 사지을 찍을 수 있도록 했습니다. 하지만 사용자는 주변에 가져가기만 해도 사진이 찍히는 것으로 생각해 어플리케이션이 정상작동이 안된다고 결론을 내렸습니다.\n\n집약만의 차이점?\n\n\n사실 이 서비스를 만드는 도중 2019년 10월 중반에 시각장애인을 위한 약학정보서비스가 나왔습니다. 인터뷰를 진행했을 때고 이러한 이야기가 나왔습니다. 이 서비스를 A리고 하겠습니다. A는 이전의 어플리케이션과 달리 이 어플리케이션은 스크린리더를 제대로 고려해 어플리케이션을 제작했습니다. 그에 대항하기 위해 집약은 A와 차별점을 두었습니다. A는 바코드를 이용해 사진으로 검색을 할 수 있지만 인터뷰를 진행하는 도중 바코드를 찾을 수 없어 A서비스의 바코드로 검색하는 기능을 사용할 수 없다고 했습니다. 집약은 이와 차별을 두기 위해 비교적 사진을 찍기 쉬운 약상자의 겉표지를 분석해 약을 검색을 할 수 있 했습니다..\n\n마무리\n집약이라는 어플리케이션을 기획하고 제작하면서 사용자에 대한 고민을 많이 하게 되었고, 이를 바탕으로 만족스러운 결과를 만들어냈고 출시를 했습니다. 결과적으로 앱개발대회에서 3등을 했고 많은 경험을 했습니다.\n\n\n"
} ,
  
  {
    "title"    : "학부생이 보는 GAN",
    "category" : "Computer Vision",
    "url"      : "/computer%20vision/2019/09/15/%ED%95%99%EB%B6%80%EC%83%9D%EC%9D%B4-%EB%B3%B4%EB%8A%94-gan/",
    "date"     : "September 15, 2019",
    "excerpt"  : "논문 링크: Ganerative Adversarial Network\nGAN은 2014년도에 나온 논문으로 현재 많은 연구에 영향을 끼치고 있고 Yann LeCun이 혁명적인 아이디어라고 극찬한했다. GAN은 Image Generation에 관한 기초 모델로 이를 활용해 늙은 사진, 언경쓴 사진 등 원하는 이미지를 만들어낼 수 있다.\nContribution\n이 논문에 Contribution은 다음과 같다.\n\n\n  이후 연구가 활발히 진행되는...",
    "content"  : "논문 링크: Ganerative Adversarial Network\nGAN은 2014년도에 나온 논문으로 현재 많은 연구에 영향을 끼치고 있고 Yann LeCun이 혁명적인 아이디어라고 극찬한했다. GAN은 Image Generation에 관한 기초 모델로 이를 활용해 늙은 사진, 언경쓴 사진 등 원하는 이미지를 만들어낼 수 있다.\nContribution\n이 논문에 Contribution은 다음과 같다.\n\n\n  이후 연구가 활발히 진행되는 GAN의 기본적인 이론적인 개념을 제시했다.\n  ganerate된 이미지는 하나의 지점으로 수렴하며 이 지점은 하나뿐인 global optimum이라는 것을 증명했다.\n    Basic Concept\n  \n\n\n“Adversarial”이라는 단어는 적대적인 이라는 뜻을 갖습니다. 논문 제목에서 알 수 있듯 이 논문에서 두 네트워크는 서로 적대적인 관계에 있으며 서로 경쟁하면서 학습해 나간다.\n\n\n\n다음 두 네트워크 Generator, Discriminator가 있다. Generator는 이미지를 만들어내는 네트워크이고 Discriminator는 이미지들이 Generator에서 만들어진 이미지인지 실제 데이터셋에 있는 실제 이미지인지 구분한다. GAN 논문에서는 이것을 지폐위조범과 경찰로 묘사했다.\n\n지폐위조범인 Generator 들킬 위험이 없는 위조지폐를 만드는 것이 목표다. 그리고 경찰인 Discriminator는 이 위조지폐를 찾아내는 것을 목표로 하고있습니다. 이러한 상황에서 각각의 네트워크들은 자신들의 성능들을 높일것이고 결과적으로 위조지폐가 완벽해서 실제지폐와 구분 할 수 없다. (p=0.5)\n\n수학적으로 접근해보면 다음과 같다. Generator는 우리가 갖고있는 data들의 distribution을 모사한다. real data를 , Generator가 입력으로 z를 받아 뽑은 Sample data를 라 하겠다. (z는 보통 Gaussian noise이다,) 만약 Discriminator가 잘 학습이 되 었다면 이 될 것이고, Generator가 학습니 잘 된다면 D(G(z))=1이 될 것이다. Discriminator는 minimum으로 Generator는 maximun으로 각각 경쟁하며 학습해서 해서 min-max problem이다.\n\nLoss Function\n위를 수식으로 정의하면 다음과 같다.\n\n\n$$min_G max_D V(D,G) = E_{x~p_{data}}[logD(x)] + E_{x~p_z(z)}[log(1-D(G(z)))]$$\n\n\n이해가 잘 안된다면 극단적으로 접근하면 됩니다. Discriminator가 학습이 잘 되었다면 가 될 것이고, 결과적으로 으로 maximum이 될 것이다. 반대로 Generator가 학습니 잘 되었다면 이 될 것이고 로 minimun이 될 것이다.\n\n\n\nGAN 논문에서 제시하고 있는 Distribution인데요. 검은색 점선은 real data distribution, 초록색 점선은 Generator distribution, 보라색 접선은 Discriminator distribution입니다. 초기상태 (a)에서는 비교적 Discriminator가 real data와 sample data를 잘 판별했으나 학습이 될 수록 real data와 sample data의 distribution이 비슷해져 Discriminator가 각각의 입력을 받았을 때, 출력하는 예측값은 0.5가 됩니다.\n\nGlobal Optimality \n\nProposition 1.\ngenerator G가 고정되었을때 최적의 dicriminator D는\n\n$$D^*_G(x)=\\frac {p_{data}(x)}{p_{data} + p_g(x)}$$\n\n\nProof.\n\n$$min_G max_D V(D,G) = E_{x~p_{data}}[logD(x)] + E_{x~p_z(z)}[log(1-D(G(z)))]$$\n$$V(G,D)=\\int_x p_{data}(x)log(D(x))dx + \\int_zp_z(z)log(1-D(G(z)))dz$$ \n$$V(G,D)=\\int_x p_{data}(x)log(D(x)) + p_z(z)log(1-D(G(z)))dz$$\n\n\n어떤 에서, 함수 는 [0, 1]범위에서 최댓값 을 갖는다.\n\n위의 식을 다음과 같이 변형할 수 있다.\n\n\n$$C(G)= max_D(G,D)$$  \n$$ = E_{x~p_{data}}[logD^*_G(x)] + E_{x~p_z(z)}[log(1-D^*_G(G(z)))]$$\n$$ = E_{x~p_{data}}[logD^*_G(x)] + E_{x~p_z(z)}[log(1-D^*_G(x))]$$\n$$ = E_{x~p_{data}}[log\\frac {p_{data}(x)}{p_{data} + p_g(x)}] \n+ E_{x~p_z(z)}[log\\frac {p_{g}(x)}{p_{data} + p_g(x)}]$$\n\n\nTheorem 1.\n의 global minimum은 오직 뿐이고, 이때 이다.\n\n직관적으로 생각했을 때 이면 이다.\n\n$$C(G)=E_{x~p_{data}}[-log2] + E_{x~p_z(z)}[-log2] = -log4$$\n\n\n이를 다음과 같이 생각할 수 있다.\n\n\n$$E_{x~data}[log\\frac {p_{data}(x)}{p_{data} + p_g(x)}] + E_{x~p_g}[log\\frac {p_{g}(x)}{p_{data} + p_g(x)}]$$ \n\n$$C(G)=-log(4) + KL(p_{data}||\\frac{p_{data} + p_g}{2}) + KL(p_{g}||\\frac{p_{data} + p_g}{2})$$\n\n$$C(G)=-log(4) + 2*JSD(p_{data}||p_{g})$$\n\n\nJensen-Shannon divergence의 범위는 이이고 그 최소점은 이다. 따라서 C(G)의 최소값은 이다.\n\nConvergence of Algorithm\nProposition 2.\n만약 G과 D가 gradient decent알고리즘으로 충분히 학습된다면 D는 다음 식에서 주어진 G과 에대해 optimum에 도달하게 된다.\n\n$$ = E_{x~p_{data}}[logD^*_G(x)] + E_{x~p_z(z)}[log(1-D^*_G(G(z)))]$$\n\n\nProof\n\nif  and  is convex in  every , then  if \n\n여기서 는 앞에서 살펴본 와 같습니다. 는 JS divergence으로 convex함수입니다. 이때 모든 D에서 이 식은 성립하므로 D의 optimal인 f_{D^*}(p_g)도 convex함수이다. 따라서 우리가 풀고자하는 문제가 convex함수이기 때문에 gradient decent알고지음을 사용하면 global optimum에 도달한다.\n\nLimitation\n앞서 살펴본 내용들을 생각한다면 혁신적인 아이디어는 맞다. 하지만 모든 초기연구가 그렇듯 한계가 있다.\n\nUnstable\n사실 Loss함수 입장에서보면 minimum이든 maximum이든 어느쪽으로가든 상관이 없다. 즉\n\n$$min_G max_D V(D,G) = E_{x~p_{data}}[logD(x)] + E_{x~p_z(z)}[log(1-D(G(z)))]$$\n\n여기서 Generator를 잘 학습시키는 것 대신 Discriminator를 잘 속이는 것으로 학습방향이 흘러갈수있다. 예를들어 mnist dataset에서 Generator는 Discriminator를 잘 속이기 위해 숫자 6만 만들어낸다고 하자. 그러면 Discriminator는 숫자 6이 나오면 Generator에서 나오는 것으로 판단하고 6이라는 이미지는 fake image라고 판단한다. 이후 Generator는 Discriminator의 판단을 속이기 위해 8을 만들어낼 것이고, 앞선 상황이 반복될 것이다.\n\n"
} 
  
]
