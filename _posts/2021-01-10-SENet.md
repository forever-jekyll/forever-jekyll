---
layout: post
title:  "학부생이 본 SENet"
date:   2021-01-10 15:41:11 +0900
tags: [Computer Vision]
comments: true
---

ImageNet Challenge에서 많은 네트워크들이 제안되었습니다. 
ResNet, VGG, Inception그 예입니다. 
하지만 이러한 네트워크들은 크기가 크다는 것이 단점이었습니다.
ILSVRC 2017에서 우승한 SENet은 squeeze-and-excitation이라는 구조로 기존 모델에서 추가적으로 성능을 높일 수 있는 Block구조를 만들었습니다.  

이 연구에서는 네트워크의 구조가 아닌 channel들의 관계들에 집중했습니다.
이전의 연구들에선 channel간의 correlation을 spatial한 정보들을 무시하고 1x1 convolutoin등과 같이 계산했습니다.
하지만 SENet에서는 global information을 이용해 명시적으로 channel간에 non-linear dependency를 계산하여 모델의 성능을 높였습니다.
Squeeze-and-excitation구조는 기존의 네트워크에 쉽게 적용하여 성능을 높일 수 있어서 더욱 유용한 구조입니다.


## Squeeze-and-excitation Block
SENet에서 comvolution들의 channeel들의 관계는 receptive field에 제한적이고 암묵적임으로 이를 명시적으로 모델링해주면 정보가 많은 특징들에 더 민감해지면서 성능이 높아진다고 말하고 있습니다.

![fig1](https://user-images.githubusercontent.com/40621030/104117748-fe1cc500-5366-11eb-9166-2a3f408bc1e4.png)

### Squeeze: Global Information Embedding
앞써 말했듯 convolutoin은 변환이 receptive field에 국한되어있어서 receptive field밖의 contextual information은 고려할 수 없습니다.
따라서 global spatial information을 channel discriptor로 만들기 위해 Squeeze연산을 수행합니다. 
이 과정은 간단히 global average pooling을 이용했습니다.  

Channel discriptor인 $$ z \cap R^n$$은 *U*를 spatial dimension으로 $$H \times W$$만큼 압축해서 얻을 수 있습니다.
따라서 c번째 체널의 z성분은 다음과 같이 계산될 수 있습니다.  

$$z_c = F_{sq}(u_c) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} u_c(i, j)$$  

*transformation U는 전체적인 이미지의 통계값인 local descriptor로 생각될 수 있고 global average pooling으로 구현할 수 있습니다.*

$$ Excitation: Adaptive Recallibration
Channel discriptor를 만들었으면 이제 체널의 관계들을 계산해줘야 합니다. 
여기서 고려해야할 사항이 두 개가 있습니다.
1. 체널들의 관계를 포착하기 위해 flexible해야합니다.
2. 체널들을 강조하기 위해 non-mutually-exclusive한 정보를 학습해야 합니다. (one-hot같은 mutually-exclusive면 안됩니다.)

따라서 여기서는 sigmoid를 사용했습니다.  
$$s=F_{ex}(z, W) = \sigma(g(z, W)) = \sigma(W_2(\delta(W_1, z))$$  

$$\delta$$는 ReLU함수를 의미하고, $$W_1 \cap R^{\frac{C}{r} \times C}$$ 그리고 $$W_2 \cap R^{C \times \frac{C}{r}}$$
모델의 복잡도와 일반화를 위해서 bottleneck이 있는 두 개의 FC Layer를 이용했고 reduction ratio r을 갖습니다.  

따라서 최종 출력은  
$$\tilde{x_c} = F_{scale}(u_c, s_c) = s_c u_c$$  
$$\tilde{X} = [\tilde{x_1}, \tilde{x_2}, ... , \tilde{x_C}]$$ 이고 $$F_{scale}(u_c, s_c)$$은 channel-wise multiplication을 의미합니다.  

*excitation operator는 channel discriptor인 z를 channel weight으로 연결해주는 연산입니다.*

이 두 가지 연산을 통해 적은 추가적인 연산량과 모델크리고 model capacity를 늘릴수 있습니다.

## SE-Inception ans SE-ResNet
![fig2](https://user-images.githubusercontent.com/40621030/104117749-ff4df200-5366-11eb-8966-e1aa2f4ccc25.png)
다음과 같이 Residual Block에 Squeeze-and-excitation구조를 적용함으로써 기존의 네트워크의 성능을 높일 수 있습니다.

## Result
SE-Inception ans SE-ResNet의 다음과 같습니다.  
![table1](https://user-images.githubusercontent.com/40621030/104117779-43d98d80-5367-11eb-82a8-4372bf1999c8.png)
그리고 실험 결과는 다음과 같습니다.  
![table2](https://user-images.githubusercontent.com/40621030/104117780-44722400-5367-11eb-8ec5-fc05c29722fe.png)

## 마치며
Squeeze-and-excitation Block은 MobileNet V3등 여러 네트워크에서 사용하는 강력한 구조입니다. 
EffientNet과 같이 새로운 네트워크 구조를 제안할까가 아니라 기존 네트워크의 부족한 점이 무엇인지 생각한 다음 실험을 하여 더 좋은 성능을 얻었다는 것이 신기했습니다.
특히 이전 연구들과 달리 channel간의 dependency를 global한 contextual information까지 확장했다는 것에 대해 의의가 있는 것 같습니다.
